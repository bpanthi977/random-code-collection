{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75a24fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from https://gymnasium.farama.org/tutorials/training_agents/reinforce_invpend_gym_v26/\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d64c56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "#%load_ext tensorboard\n",
    "#wandb.init(project=\"cartpole-v1\", entity=\"bpanthi977\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e67dd8dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, steps=100):\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0\n",
    "    total_episodes = 0\n",
    "    for _ in range(steps):\n",
    "        action = agent.action(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        # env.render()\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            total_episodes += 1\n",
    "            \n",
    "    return total_reward/total_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cba095b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def action(self, state):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aea5d8c-1008-486f-8fdb-fa317c5a950a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.333333333333336"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(RandomAgent(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2040aeb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPoleAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleAgent, self).__init__()\n",
    "        input_dim = 4\n",
    "        out_dim = 2\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def action(self, state):\n",
    "        probs = self.forward(torch.tensor(state).to(device))\n",
    "        action = np.random.choice([0,1], p=probs.detach().cpu().numpy())\n",
    "        self.action_probs = probs\n",
    "        \n",
    "        return action\n",
    "\n",
    "device = torch.device('mps')   \n",
    "def reset_network():\n",
    "    global network, optim, total_episodes, writer\n",
    "    network = CartPoleAgent().to(device)\n",
    "    optim = torch.optim.Adam(network.parameters())\n",
    "    total_episodes = 0\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "reset_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7266d560",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.666666666666668"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(network, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2385a703",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GAMMA=0.99\n",
    "ENTROPY_BETA=0.1\n",
    "def train_reinforce(steps):\n",
    "    global total_episodes\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    total_reward = 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    par = []\n",
    "    \n",
    "    def train(par):\n",
    "        # REINFORCE Update\n",
    "        # compute returns in backward order and compute loss\n",
    "        g_t = 0\n",
    "        loss = 0\n",
    "        for t in range(len(par)-1, -1, -1):\n",
    "            prob, action, reward = par[t]\n",
    "            g_t = reward + GAMMA * g_t\n",
    "\n",
    "            log_prob = torch.log(prob)\n",
    "            # L = - G ln \\pi(a)\n",
    "            loss += - g_t * log_prob[action]\n",
    "            ## L = entropy penalty\n",
    "            entropy = - (prob * log_prob).sum()         # H = - \\sum p_i log p_i\n",
    "            loss += - ENTROPY_BETA * entropy          # increase entropy \n",
    "        \n",
    "        if g_t == 0:\n",
    "            return \n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "            \n",
    "    for step in range(steps):\n",
    "        action = network.action(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        par.append([network.action_probs, action, reward])\n",
    "        \n",
    "        total_reward += reward\n",
    "        episode_steps += 1\n",
    "        if terminated or truncated:\n",
    "            train(par)\n",
    "            par = []\n",
    "            observation, info = env.reset()\n",
    "            total_episodes += 1\n",
    "            writer.add_scalar(\"Reward/Episode\", episode_steps, total_episodes)\n",
    "            episode_steps = 0\n",
    "            \n",
    "    train(par)\n",
    "            \n",
    "    avg_reward = total_reward/total_episodes\n",
    "    writer.add_hparams({'entropy': ENTROPY_BETA, 'gamma': GAMMA, 'baseline': False}, {'avg_reward': avg_reward})\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ce1f3f-3b64-405b-8dcb-6b76bbfb5876",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.918454935622314"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_network()\n",
    "train_reinforce(30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6703812-389c-4bea-ad71-5616b7d2d973",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(network, 1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/opt/homebrew/opt/python@3.11/bin/python3.11",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "name": "02 CartPole - 01 - REINFORCE Policy Network.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
